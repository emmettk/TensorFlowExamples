{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation example with numpy on MNIST\n",
    "E. Krupczak - 19 Aug 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last node goes to loss, so d(loss)/d(final node) = 1\n",
    "$$\\frac{d(loss)}{d(node)} = \\text{node weight}$$\n",
    "i.e. for an affine layer:\n",
    "$$Loss = x_1+x_2+x_3+x_4$$\n",
    "$$\\frac{dLoss}{dx_1} = 1$$\n",
    "and for a non-linear layer:\n",
    "$$Loss = \\tanh(x_1)$$\n",
    "$$\\frac{dLoss}{dx_1}= \\frac{d\\tanh(x_1)}{dx_1}$$\n",
    "\n",
    "Use chain rule to combine layers, eg. $\\alpha$, $\\beta$:\n",
    "$$\\frac{dL}{d\\alpha} = \\frac{d\\beta}{d\\alpha}\\cdot\\frac{dL}{d\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for every kind of layer:\n",
    "- Affine layer\n",
    "    - Matrix multiplication layer\n",
    "    - Bias layer\n",
    "- Nonlinear layer (eg. tanh)\n",
    "- Loss/squared-error layer\n",
    "- Softmax cross entropy layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward_pass(self, i):\n",
    "        output = i\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward_pass(output)\n",
    "        return output\n",
    "    \n",
    "    def backward_pass(self, gradient, learning_rate):\n",
    "        output = gradient\n",
    "        for layer in self.layers[::-1]:\n",
    "            output = layer.backward_pass(output, learning_rate)\n",
    "        return output\n",
    "\n",
    "class LinearLayer:\n",
    "    '''\n",
    "    Takes a vector of length i, returns a vector of length j.\n",
    "    Network weights stored as \"weights\", a j by i matrix.\n",
    "    '''\n",
    "    def __init__(self, weights):\n",
    "        self.weights = weights\n",
    "    \n",
    "    def forward_pass(self, i):\n",
    "        self.last_input = i        \n",
    "        return self.weights @ i\n",
    "    \n",
    "    def backward_pass(self, gradient, learning_rate):\n",
    "        self.weights -= learning_rate * gradient @ self.last_input.T\n",
    "        return self.weights.T @ gradient\n",
    "    \n",
    "class BiasLayer:\n",
    "    '''\n",
    "    Takes a vector of length i, returns a vector of length i.\n",
    "    Biases stored in a vector of length i.\n",
    "    '''\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        \n",
    "    def forward_pass(self, i):\n",
    "        self.last_input = i\n",
    "        return i + self.bias\n",
    "    \n",
    "    def backward_pass(self, gradient, learning_rate):\n",
    "        self.bias -= learning_rate * gradient\n",
    "        return gradient\n",
    "    \n",
    "class TanhLayer:\n",
    "    '''\n",
    "    Takes a vector of length i, returns a vector of length i.\n",
    "    '''\n",
    "    def forward_pass(self, i):\n",
    "        self.last_input = i\n",
    "        return np.tanh(i)\n",
    "    \n",
    "    def backward_pass(self, gradient, learning_rate):\n",
    "        '''\n",
    "        gradient: gradient of loss function at output\n",
    "        returns loss function at input\n",
    "        '''\n",
    "        return gradient / np.cosh(self.last_input)**2\n",
    "    \n",
    "class SoftmaxLayer:\n",
    "    '''\n",
    "    Softmax: exponentiate and normalize.\n",
    "    '''\n",
    "    def forward_pass(self, i):\n",
    "        self.last_input = i\n",
    "        exp = np.exp(i)\n",
    "        return exp / np.sum(exp)\n",
    "    \n",
    "    def backward_pass(self, gradient, learning_rate):\n",
    "        deriv = \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tanh layer:\n",
    "$y =\\tanh(x)$ on the forwards pass. \n",
    "On the backwards pass, then, to take $\\frac{dL}{dy}$ to $\\frac{dL}{dx}$, we use the chain rule:\n",
    "$$\\frac{dL}{dy}\\cdot\\frac{dy}{dx}\\bigg|_{x_0} = \\frac{dL}{dx}\\bigg|_{x_0}$$\n",
    "\n",
    "For the softmax layer:\n",
    "$$y_j = \\frac{e^x_j}{\\sum_{k}e^x_k}$$\n",
    "On the backwards pass:\n",
    "$$\\frac{dy_i}{dx_j} = x_i(\\delta_{ij}-x_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputvec = np.random.randn(5,1)\n",
    "weights = np.random.randn(5,5)\n",
    "biases = np.ones([5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "linlayer = LinearLayer(weights)\n",
    "biaslayer = BiasLayer(biases)\n",
    "tanhlayer = TanhLayer()\n",
    "network = Network([linlayer, biaslayer, tanhlayer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.95741829],\n",
       "       [ 0.99770961],\n",
       "       [ 0.99842318],\n",
       "       [ 0.93810152],\n",
       "       [ 0.77690749]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.forward_pass(inputvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03072998],\n",
       "       [ 0.08828204],\n",
       "       [-0.08143696],\n",
       "       [-0.06928156],\n",
       "       [-0.14483833]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.backward_pass(inputvec, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
